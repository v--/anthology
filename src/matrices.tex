\subsection{Matrices}\label{subsec:matrices}

\begin{definition}\label{def:array}
  Let \( X \) be any nonempty set. A \( k \)-dimensional \Def{array} \( A \) of shape \( (n_1, \ldots, n_k) \) over \( X \) is a function of type
  \begin{equation*}
    A: \{ 1, 2, \ldots, n_1 \} \times \ldots \times \{ 1, 2, \ldots, n_k \} \to X.
  \end{equation*}

  In particular,
  \begin{defenum}
    \DItem{def:array/matrix} two-dimensional arrays of shape \( n, m \) are usually called \Def{matrices}. An \( n, m \)-matrix \( A \) is denoted as
    \begin{equation*}
      A = \{ a_{i,j} \}_{i,j=1}^{n,m}
    \end{equation*}
    or graphically as tables
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m}
      \end{pmatrix}.
    \end{equation*}

    The elements \( a_{1,1}, \ldots, a_{\min{n, m}, \min{n, m}} \) of a matrix are called its \Def{main diagonal}.

    \DItem{def:array/square_matrix} If \( n = m \), we call the matrix a \Def{square matrix} of order \( n \).

    \DItem{def:array/column_vector} matrices with only one column are called \Def{column matrices}:
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} \\
        \vdots  \\
        a_{n,1}
      \end{pmatrix}.
    \end{equation*}

    \DItem{def:array/row_vector} matrices with only one row are called \Def{row matrices}:
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,m}
      \end{pmatrix}.
    \end{equation*}

    \DItem{def:array/vector} one-dimensional arrays are called simply \Def{vectors} or tuples\Tinyref{def:cartesian_product} and are usually written as either column vectors or row vectors.
  \end{defenum}
\end{definition}

\begin{remark}\label{remark:arrays_vs_tensors}
  Multidimensional arrays, as defined in \cref{def:array}, are often called tensors, especially in machine learning where they are often used. This is a confusing practice since tensors (see \cref{def:module_tensor_product}) are defined in a coordinate-independent fashion.

  A single tensor can be represented by different arrays and the same array can represent multiple tensors.
\end{remark}

\begin{definition}\label{def:block_matrix}
  A \Def{block matrix} is a \enquote{matrix of matrices}, that is, a matrix of the form
  \begin{equation*}
    \begin{pmatrix}
      A_{1,1} & A_{1,2} & \cdots & A_{1,m} \\
      A_{2,1} & A_{2,2} & \cdots & A_{2,m} \\
      \vdots  & \vdots  & \ddots & \vdots \\
      A_{n,1} & A_{n,2} & \cdots & A_{n,m}
    \end{pmatrix},
  \end{equation*}
  where all \( A_{i,j} \) are matrices of compatible dimensions.

  We usually write the block matrix
  \begin{equation*}
    \begin{pmatrix}
      A      & \cdots & B \\
      \vdots & \ddots & \vdots \\
      C      & \cdots & D
    \end{pmatrix}
  \end{equation*}
  as
  \begin{equation*}
    \begin{BlockMatrix}{ccc|c|ccc}
      a_{1,1}   & \cdots & a_{1,m_A}   & \cdots & b_{1,1}   & \cdots & b_{1,m_B} \\
      \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
      a_{n_A,1} & \cdots & a_{n_A,m_A} & \cdots & b_{n_B,1} & \cdots & b_{n_B,m_B} \\
      \hline
      \vdots    & \vdots & \vdots      & \ddots & \vdots    & \vdots & \vdots \\
      \hline
      c_{1,1}   & \cdots & c_{1,m_C}   & \cdots & d_{1,1}   & \cdots & d_{1,m_D} \\
      \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
      c_{n_C,1} & \cdots & c_{n_C,m_C} & \cdots & d_{n_D,1} & \cdots & d_{n_D,m_D} \\
    \end{BlockMatrix}.
  \end{equation*}

  Given any matrix \( A = \{ a_{i,j} \}_{i,j=1}^{n,m} \), we sometimes consider its block matrix of \Def{rows}
  \begin{equation*}
    \begin{BlockMatrix}{c}
      a_{1,-} \\
      \hline
      a_{2,-} \\
      \hline
      \vdots \\
      \hline
      a_{n,-}
    \end{BlockMatrix},
  \end{equation*}
  consisting of row vectors, and its block matrix of of \Def{columns}
  \begin{equation*}
    \begin{BlockMatrix}{c|c|c|c}
      a_{-,1} & a_{-,2} & \cdots & a_{-,m},
    \end{BlockMatrix}
  \end{equation*}
  consisting of column vectors.
\end{definition}

\begin{definition}\label{def:left_module_of_tuples}
  Let \( R \) be a semiring\Tinyref{def:semiring}. Let \( R^n \) be the set of all \( n \)-tuples\Tinyref{def:array/vector} over \( R \), that is,
  \begin{equation*}
    R^n = R \times R \times \cdots \times R.
  \end{equation*}

  It is customary to denote elements of \( R^n \) by
  \begin{equation*}
    x = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}.
  \end{equation*}
  rather than
  \begin{equation*}
    x = (x_1, \ldots, x_n).
  \end{equation*}

  This highlights that \( R^n \) is usually treated as a space of column vectors.

  Define the operations
  \begin{align*}
    &+: R^n \times R^n \to R^n
    \\
    &\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
    +
    \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}
    =
    \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}
    \\
    \\
    &\cdot: R \times R^n \to R^n
    \\
    &\lambda \cdot \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
    =
    \begin{pmatrix} \lambda x_1 \\ \vdots \\ \lambda x_n \end{pmatrix}.
  \end{align*}

  With these operations defined, \( R^n \) becomes a semiring module\Tinyref{def:left_module}.

  In particular, if \( R \) is a field\Tinyref{def:semiring/field}, \( R^n \) is a vector space\Tinyref{def:vector_space} and we refer to it as a \Def{tuple space}. We are usually only concerned with the vector spaces \( \R^n \) and \( \BB{C}^n \).
\end{definition}

\begin{proposition}\label{def:matrix_spaces_are_tuple_spaces}
  The vector spaces \( F^{n \times m} \) and \( F^{nm} \) are isomorphic with an isomorphism defined by \cref{def:double_index_maps}.
\end{proposition}

\begin{remark}\label{remark:vector_spaces_of_tuples_and_matrices}
  \Cref{thm:finite_dimensional_spaces_are_isomorphic} provides a justification for working with vector spaces of tuples instead of arbitrary vector spaces.

  \Cref{thm:finite_dimensional_operators_are_isomorphic_to_matrices} provides a justification for working with vector spaces of matrices instead of arbitrary spaces of linear operators.
\end{remark}

\begin{theorem}\label{thm:finite_dimensional_spaces_are_isomorphic}
  Every \( n \)-dimensional\Tinyref{def:vector_space_dimension} vector space\Tinyref{def:vector_space} over the field \( F \) is isomorphic to \( F^n \)\Tinyref{def:left_module_of_tuples}.
\end{theorem}
\begin{proof}
  Let \( V \) be an arbitrary \( n \)-dimensional vector space over \( F \). Since a basis of \( V \) exists\AOC by \cref{thm:all_vector_spaces_are_free_left_modules}, fix a basis and fix an ordering \( b_1, \ldots, b_n \) of the basis vectors. Denote the projection maps\Tinyref{def:left_module_basis_projection} by \( \pi_{b_i} \).

  Define the function
  \begin{align*}
    &L: V \to F^n \\
    &L(x) \coloneqq (\pi_{b_1}(x), \ldots, \pi_{b_n}(x))
  \end{align*}
  that maps a vector \( x \in V \) into an \( n \)-tuple of the projections of \( x \) along the ordered basis \( b_1, \ldots, b_n \). It is linear since, by \cref{thm:left_module_basis_projections_are_linear}, the projections are linear.

  Now define the inverse function
  \begin{align*}
    &P: F^n \to V \\
    &P(y_1, \ldots, y_n) \coloneqq \sum_{i=1}^n y_i b_i,
  \end{align*}
  which is obviously linear.

  The composition of \( L \) and \( P \) is the identity mapping on \( V \). Indeed, for any \( x \in V \),
  \begin{equation*}
    (P \circ L)(x)
    =
    P(\pi_{b_1}(x), \ldots, \pi_{b_n}(x))
    =
    \sum_{i=1}^n \pi_{b_i}(x) b_i
    =
    x.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:algebra_of_matrices}
  Denote by \( R^{n \times m} \) the set of \( n, m \)-matrices over the semiring \( R \). We define three operations on matrices:

  \begin{description}
    \DItem{def:algebra_of_matrices/addition} We define \Def{matrix addition} as
    \begin{align*}
      &+: R^{n,m} \times R^{n,m} \to R^{n,m} \\
      &\begin{pmatrix}
        a_{1,1} & \cdots & a_{1,m} \\
        \vdots  & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n,m}
      \end{pmatrix}
      +
      \begin{pmatrix}
        b_{1,1} & \cdots & b_{1,m} \\
        \vdots  & \ddots & \vdots \\
        b_{n,1} & \cdots & b_{n,m}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        a_{1,1} + b_{1,1} & \cdots & a_{1,m} + b_{1,m} \\
        \vdots            & \ddots & \vdots \\
        a_{n,1} + b_{n,1} & \cdots & a_{n,m} + b_{n,m}
      \end{pmatrix}
    \end{align*}
    \DItem{def:algebra_of_matrices/scalar_multiplication} We define \Def{scalar multiplication} as
    \begin{align*}
      &\cdot: R \times R^{n,m} \to R^{n,m} \\
      &\lambda \cdot \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,m} \\
        \vdots  & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n,m}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        \lambda a_{1,1} & \cdots & \lambda a_{1,m} \\
        \vdots          & \ddots & \vdots \\
        \lambda a_{n,1} & \cdots & \lambda a_{n,m}
      \end{pmatrix}
    \end{align*}

    \DItem{def:algebra_of_matrices/matrix_multiplication} We define \Def{matrix multiplication} in two steps. The complexity of the definition is justified by \cref{thm:finite_dimensional_operators_are_isomorphic_to_matrices}. First, if \( a \in R^{1,n} \) is a row vector\Tinyref{def:array/row_vector} and \( b \in R^{n,1} \) is a column vector\Tinyref{def:array/column_vector}, we define their \Def{inner product} to be
    \begin{equation}
      a \cdot b \coloneqq \sum_{i=1}^n a_i b_i.
    \end{equation}

    We can now define matrix multiplication as
    \begin{align*}
      &\circ: R^{n,m} \times R^{m,k} \to R^{n,k} \\
      &\begin{BlockMatrix}{c}
        a_{1,-} \\
        \hline
        a_{2,-} \\
        \hline
        \vdots \\
        \hline
        a_{n,-}
      \end{BlockMatrix}
      \circ
      \begin{BlockMatrix}{c|c|c|c}
        \scriptstyle{b_{-,1}} & \scriptstyle{b_{-,2}} & \cdots & \scriptstyle{b_{-,m}}
      \end{BlockMatrix}
      \coloneqq
      \begin{pmatrix}
        a_{1,-} \cdot b_{-,1} & a_{1,-} \cdot b_{-,2} & \vdots & a_{1,-} \cdot b_{-,m} \\
        a_{2,-} \cdot b_{-,1} & a_{2,-} \cdot b_{-,2} & \vdots & a_{2,-} \cdot b_{-,m} \\
        \vdots                & \vdots                & \ddots & \vdots \\
        a_{n,-} \cdot b_{-,1} & a_{n,-} \cdot b_{-,2} & \cdots & a_{n,-} \cdot b_{-,m}
      \end{pmatrix}.
    \end{align*}
  \end{description}

  With addition\Tinyref{def:algebra_of_matrices/addition} and scalar multiplication\Tinyref{def:algebra_of_matrices/scalar_multiplication}, \( R^{n \times m} \) becomes a semiring module\Tinyref{def:left_module}.

  If \( n = m \), we can add matrix multiplication\Tinyref{def:algebra_of_matrices/matrix_multiplication} to the module \( R^{n \times n} \) so that it becomes an algebra\Tinyref{def:algebra_over_ring} over \( R \). If \( R \) is unital, the identity matrix of order \( n \) is the identity with respect to matrix multiplication.
\end{definition}

\begin{example}\label{ex:matrix_multiplication_is_noncommutative}
  The matrix algebra \( R^{n \times n} \) is a noncommutative ring. Consider the following example:
  \begin{align*}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix}
    \\
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      0 & 0
    \end{pmatrix}
  \end{align*}
\end{example}

\begin{proposition}\label{thm:finite_dimensional_operators_are_isomorphic_to_matrices}
  Fix a dioid \( R \). The matrix vector space\Tinyref{def:algebra_of_matrices} \( R^{n \times m} \) is isomorphic to the vector space of all linear maps \( \hom(R^m, R^n) \) (note that the maps are from \( R^m \) to \( R^n \)).

  If \( n = m \), the algebra \( R^{n \times n} \) with matrix multiplication as vector multiplication is isomorphic to \( \End(R^n) \) with function composition as vector multiplication. In particular, this justifies using juxtaposition for application of linear functions, e.g. \( Lx \) rather than \( L(x) \).
\end{proposition}
\begin{proof}
  Let \( L: R^m \to R^n \) be a linear map and let \( e_1, \ldots, e_m \) be the basis vectors in \( R^m \). Denote by \( \pi_i, i = 1, \ldots, m \) be the basis projections\Tinyref{def:left_module_basis_projection}. We construct a matrix as follows:
  \begin{equation*}
    A_L \coloneqq \begin{BlockMatrix}{c|c|c|c}
      L(e_1) & L(e_2) & \cdots & L(e_m)
    \end{BlockMatrix}
  \end{equation*}

  Conversely, given a matrix \( A \in R^{n \times m} \), we define the linear map
  \begin{equation*}
    \hat L_A(x) \coloneqq Ax
  \end{equation*}
  by left multiplication of a vector with \( A \).

  It remains to show that these are mutually inverse. Let \( L: R^m \to R^n \) and \( x \in R^m \). We have
  \begin{equation*}
    L_{A_L}(x) = A_L x = \sum_{i=1}^m \pi_i(x) L(e_i) = L\left(\sum_{i=1}^m \pi_i(x) e_i \right) = L(x).
  \end{equation*}

  Conversely, let \( A \in R^{n \times m} \) and \( x \in R^m \). We have
  \begin{align*}
    A_{L_A} x
    &=
    \begin{BlockMatrix}{c|c|c|c}
      L_A(e_1) & L_A(e_2) & \cdots & L_A(e_m)
    \end{BlockMatrix}
    x
    = \\ &=
    \begin{BlockMatrix}{c|c|c|c}
      A e_1 & A e_2 & \cdots & A e_m
    \end{BlockMatrix}
    x
    = \\ &=
    \sum_{i=1}^m \pi_i(x) A e_i
    = \\ &=
    A \left( \sum_{i=1}^m \pi_i(x) e_i \right)
    = \\ &=
    Ax.
  \end{align*}

  It trivially follows that linear function composition corresponds to matrix multiplication.
\end{proof}

\begin{definition}\label{def:matrix_determinant}
  Let \( A \in R^{n \times n} \) be a square matrix over a semiring \( R \). We define its determinant with induction on \( n \):
  \begin{itemize}
    \item If \( n = 1 \), define
    \begin{equation*}
      \det \begin{pmatrix} a_{1,1} \end{pmatrix} \coloneqq a_{1,1}.
    \end{equation*}

    \item Having already defined determinants for matrix orders \( < n \), define
    \begin{equation*}
      \det \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,n}
      \end{pmatrix}
      =
      \sum_{i=1}^n (-1)^i a_{1,i} \begin{pmatrix}
        a_{2,1} & \cdots & a_{2, i - 1} & a_{2, i + 1} & \cdots & a_{2,n} \\
        a_{3,1} & \cdots & a_{3, i - 1} & a_{3, i + 1} & \cdots & a_{3,n} \\
        \vdots  & \ddots & \vdots       & \vdots       & \ddots & \vdots \\
        a_{n,1} & \cdots & a_{n, i - 1} & a_{n, i + 1} & \cdots & a_{n,n}
      \end{pmatrix}
    \end{equation*}

    This is usually called \Def{row expansion}.
  \end{itemize}

  If a matrix has determinant zero, we say that it is \Def{singular}.
\end{definition}

\begin{definition}\label{def:inverse_matrix}
  Let \( A \) be a square matrix of order \( n \) over a dioid \( R \). We say that \( B \) is an \Def{inverse matrix} of \( A \) if
  \begin{equation*}
    AB = BA = E_n.
  \end{equation*}

  An inverse matrix, if it exists, is unique. We denote this inverse of \( A \) by \( A^{-1} \).

  The set of all invertible matrices of order \( n \) over \( R \) is called the \Def{general linear group} and is denoted by \( \GL_n(R) \). It forms a group with respect to matrix multiplication.

  We also consider the \Def{special linear group} \( \SL_n(R) \) of matrices with determinant\Tinyref{def:matrix_determinant} \( 1 \).
\end{definition}
\begin{proof}
  The inverse is unique by \cref{thm:group_properties/unique_inverse}.
\end{proof}

\begin{proposition}\label{def:matrix_is_invertible_iff_singular}
  Fix a dioid \( R \). The general linear group \( \GL_n(R) \) is isomorphic to the group of all invertible linear transformations over \( R^n \) under composition.
\end{proposition}
\begin{proof}
  Follows from \cref{thm:finite_dimensional_operators_are_isomorphic_to_matrices}.
\end{proof}

\begin{definition}\label{def:orthogonal_matrix}
  Let \( R \) be a dioid. We say that the square matrix \( A \) is \Def{orthogonal} if \( A^T = A^{-1} \). The set of all orthogonal matrices of order \( n \) is called the orthogonal group \( \Orth_n(R) \) and is a subgroup of \( \GL_n(R) \).
\end{definition}

\begin{definition}\label{def:unitary_matrix}
  We say that the complex square matrix \( A \) is \Def{unitary} if \( A^\dagger = A^{-1} \). The set of all unitary matrices of order \( n \) is called the unitary group \( \Unitary_n \) and is a subgroup of \( \GL_n(\BB{C}) \).
\end{definition}

\begin{definition}\label{def:matrix_column_and_row_space}
  Fix a matrix \( A \in R^{n \times m} \) over a semiring \( R \). We define its \Def{row space} as
  \begin{equation*}
    \Span \{ a_{i,-} \colon i = 1, \ldots, n \}
  \end{equation*}
  and its \Def{column space} as
  \begin{equation*}
    \Span \{ a_{-,j} \colon j = 1, \ldots, m \}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:matrix_transpose}
  Let \( A = \{ a_{i,j} \}_{i,j=1}^{n,m} \) be a matrix. We define its \Def{transpose matrix} by \Def{flipping it over its main diagonal}, that is,
  \begin{equation*}
    A^T \coloneqq \begin{pmatrix}
      a_{1,1} & a_{2,1} & \cdots & a_{n,1} \\
      a_{1,2} & a_{2,2} & \cdots & a_{n,2} \\
      \vdots  & \vdots  & \ddots & \vdots \\
      a_{1,m} & a_{2,m} & \cdots & a_{n,m}
    \end{pmatrix}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:symmetric_matrix}
  A square matrix \( A \) is said to be \Def{symmetric} if \( A = A^T \).
\end{definition}

\begin{definition}\label{def:matrix_conjugate_transpose}
  Let \( A \) be a complex matrix. We define its \Def{conjugate transpose matrix} as
  \begin{equation*}
    A^\dagger \coloneqq \begin{pmatrix}
      \Ol{a_{1,1}} & \Ol{a_{2,1}} & \cdots & \Ol{a_{n,1}} \\
      \Ol{a_{1,2}} & \Ol{a_{2,2}} & \cdots & \Ol{a_{n,2}} \\
      \vdots  & \vdots  & \ddots & \vdots \\
      \Ol{a_{1,m}} & \Ol{a_{2,m}} & \cdots & \Ol{a_{n,m}}
    \end{pmatrix}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:hermitian_matrix}
  If \( A \) is a complex matrix, we say that it is Hermitian if \( A = A^\dagger \).
\end{definition}

\begin{proposition}\label{thm:dual_linear_operator_matrix_transpose}
  Let \( L: F^m \to F^n \) be a linear operator and let \( L^*: {F^n}^* \to {F^m}^* \) be its dual operator\Tinyref{def:dual_linear_operator}.

  If \( A \in F^{n \times m} \) is the matrix of \( L \), then its transpose\Tinyref{def:matrix_transpose} \( A^T \) is the matrix of \( L^* \) when regarding \( L^* \) as an operator acting on column vectors.
\end{proposition}
\begin{proof}
  Let \( l \in {F^n}^* \) be a linear functional regarded as a function and \( \vec l \) be the same functional regarded as a column vector\Tinyref{remark:finite_dimensional_dual_space_isomorphism}. We have
  \begin{equation*}
    L^*(l)
    =
    l \circ L
    =
    (x \mapsto l(L(x)))
    =
    (x \mapsto \vec l^T Ax)
    =
    \vec l^T A.
  \end{equation*}

  Thus
  \begin{equation*}
    L^*(l) = A^T \vec l,
  \end{equation*}
  i.e. the matrix \( A^T \) corresponds to the dual operator \( L^* \).
\end{proof}

\begin{proposition}\label{thm:column_and_row_spaces_are_images}
  Fix a semiring \( R \). Let \( L: R^m \to R^n \) be a linear map and let \( A \in R^{n \times m} \) be the corresponding matrix\Tinyref{thm:finite_dimensional_operators_are_isomorphic_to_matrices}. The column space\Tinyref{def:matrix_column_and_row_space} of \( A \) is isomorphic to the image \( \Img(L) \) and the row space is isomorphic to \( \Img(L^*) \).
\end{proposition}
\begin{proof}
  The column space of \( A \) lies within \( R^{n \times 1} \), which is isomorphic to \( R^n \). We will assume that it is a subset of \( R^n \) and will prove that it is equal to \( \Img(L) \).

  Denote by \( e_1, \ldots, e_m \) the basis of \( R^m \). The \( j \)-th column \( a_{-,j} \) of \( A \) can be represented as
  \begin{equation*}
    A e_j = a_{-,j}.
  \end{equation*}

  Thus \( a_{-,j} \in \Img(L), j = 1, \ldots, m \). Since \( \Img(L) \) is a linear subspace of \( R^n \), it contains the linear span of any finite collection of its vectors. Consequently, the column space of \( A \) is a subspace of \( \Img(L) \).

  To see the converse, let \( x \in \R^m \). We have
  \begin{equation*}
    L(x) = Ax = \sum_{j=1}^m x_i A e_j = \sum_{j=1}^m a_{-,j}.
  \end{equation*}

  Hence the image of any vector \( x \) under \( L \) is a linear combination of the columns of \( A \).

  Thus proves that the column space of \( A \) is equal to \( \Img(L) \).

  The proof that the row space is isomorphic to \( \Img(L^*) \) is identical, noting that \( A^T \) corresponds to \( L^* \) by \cref{thm:dual_linear_operator_matrix_transpose}.
\end{proof}
