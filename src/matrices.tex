\subsection{Matrices}\label{subsec:matrices}

\begin{definition}\label{def:array}
  Let \( X \) be any nonempty set. A \( k \)-dimensional \textbf{array} \( A \) of shape \( (n_1, \ldots, n_k) \) over \( X \) is a function of type
  \begin{equation*}
    A: [1, 2, \ldots, n_1] \times \ldots \times [1, 2, \ldots, n_k] \to X.
  \end{equation*}

  In particular, 1-dimensional arrays of shape \( n \) are usually called \textbf{tuples} (since they are identical to tuples as defined in \cref{def:cartesian_product}) or \textbf{vectors} (since, if \( X = \BB{R} \), they are identical to vectors in \( \BB{R}^n \)).

  2-dimensional arrays of shape \( n, m \) are usually called \textbf{matrices}.
\end{definition}

\begin{remark}\label{remark:arrays_vs_tensors}
  Multidimensional arrays, as defined in \cref{def:array}, are often called tensors, especially in machine learning where they are often used. This is a confusing practice since tensors (see \cref{def:module_tensor_product}) are defined in a coordinate-independent fashion.

  A single tensor can be represented by different arrays and the same array can represent multiple tensors.
\end{remark}
