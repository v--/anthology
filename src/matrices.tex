\subsection{Matrices}\label{subsec:matrices}

\begin{definition}\label{def:array}
  Let \( X \) be any nonempty set. A \( k \)-dimensional \Def{array} \( A \) of shape \( (n_1, \ldots, n_k) \) over \( X \) is a function of type
  \begin{equation*}
    A: \{ 1, 2, \ldots, n_1 \} \times \ldots \times \{ 1, 2, \ldots, n_k \} \to X.
  \end{equation*}

  In particular,
  \begin{defenum}
    \DItem{def:array/matrix} two-dimensional arrays of shape \( n, m \) are usually called \Def{matrices}. An \( n, m \)-matrix \( A \) is denoted as
    \begin{equation*}
      A = \{ a_{i,j} \}_{i,j=1}^{n,m}
    \end{equation*}
    or graphically as tables
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m}
      \end{pmatrix}.
    \end{equation*}

    \DItem{def:array/column_vector} matrices with only one column are called \Def{column matrices}:
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} \\
        \vdots  \\
        a_{n,1}
      \end{pmatrix}.
    \end{equation*}

    \DItem{def:array/row_vector} matrices with only one row are called \Def{row matrices}:
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,m}
      \end{pmatrix}.
    \end{equation*}

    \DItem{def:array/vector} one-dimensional arrays are called simply \Def{vectors} or tuples\Tinyref{def:cartesian_product} and are usually written as either column vectors or row vectors.
  \end{defenum}
\end{definition}

\begin{remark}\label{remark:arrays_vs_tensors}
  Multidimensional arrays, as defined in \cref{def:array}, are often called tensors, especially in machine learning where they are often used. This is a confusing practice since tensors (see \cref{def:module_tensor_product}) are defined in a coordinate-independent fashion.

  A single tensor can be represented by different arrays and the same array can represent multiple tensors.
\end{remark}

\begin{definition}\label{def:module_of_tuples}
  Let \( R \) be a semiring\Tinyref{def:ring}. Let \( R^n \) be the set of all \( n \)-tuples\Tinyref{def:array/vector} over \( R \), that is,
  \begin{equation*}
    R^n = R \times R \times \cdots \times R.
  \end{equation*}

  It is customary to denote elements of \( R^n \) by
  \begin{equation*}
    x = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}.
  \end{equation*}
  rather than
  \begin{equation*}
    x = (x_1, \ldots, x_n).
  \end{equation*}

  Define the operations
  \begin{align*}
    &+: R^n \times R^n \to R^n
    \\
    &\begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
    +
    \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}
    =
    \begin{pmatrix} x_1 + y_1 \\ \vdots \\ x_n + y_n \end{pmatrix}
  \end{align*}
  and
  \begin{align*}
    &\cdot: R \times R^n \to R^n
    \\
    &\lambda \cdot \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}
    =
    \begin{pmatrix} \lambda x_1 \\ \vdots \\ \lambda x_n \end{pmatrix}.
  \end{align*}

  With these operations defined, \( R^n \) becomes a semiring module\Tinyref{def:module_over_ring}.

  In particular, if \( R \) is a field\Tinyref{def:field}, \( R^n \) is a vector space\Tinyref{def:vector_space} and we refer to it as a \Def{tuple space}. We are usually only concerned with the vector spaces \( \R^n \) and \( \BB{C}^n \).
\end{definition}

\begin{definition}\label{def:module_of_matrices}
  We denote analogous to \cref{def:module_of_tuples} operations on the set \( R^{n \times m} \) of \( n, m \)-matrices over \( R \) to obtain the semiring module of matrices over \( R \). If \( R \) is a vector space, we refer to \( R^{n \times m} \) as \Def{matrix spaces}.
\end{definition}

\begin{proposition}\label{def:matrix_spaces_are_tuple_spaces}
  The vector spaces \( F^{n \times m} \) and \( F^{nm} \) are isomorphic with an isomorphism defined by \cref{def:double_index_maps}.
\end{proposition}

\begin{remark}\label{remark:vector_spaces_of_tuples_and_matrices}
  \Cref{thm:finite_dimensional_spaces_are_isomorphic} provides a justification for working with vector spaces of tuples instead of arbitrary vector spaces.

  \Cref{thm:finite_dimensional_operators_are_isomorphic} provides a justification for working with vector spaces of matrices instead of arbitrary spaces of linear operators.
\end{remark}

\begin{theorem}\label{thm:finite_dimensional_spaces_are_isomorphic}
  Every \( n \)-dimensional\Tinyref{def:vector_space_dimension} vector space\Tinyref{def:vector_space} over the field \( F \) is isomorphic\Tinyref{def:category_of_vector_spaces} to \( F^n \)\Tinyref{def:module_of_tuples}.
\end{theorem}
\begin{proof}
  Let \( V \) be an arbitrary \( n \)-dimensional vector space over \( F \). Since a basis of \( V \) exists\AOC by \cref{thm:all_vector_spaces_are_free_modules}, fix a basis and fix an ordering \( b_1, \ldots, b_n \) of the basis vectors. Denote the projection maps\Tinyref{def:free_module_basis_projection} by \( \pi_{b_i} \).

  Define the function
  \begin{align*}
    &L: V \to F^n \\
    &L(x) \coloneqq (\pi_{b_1}(x), \ldots, \pi_{b_n}(x))
  \end{align*}
  that maps a vector \( x \in V \) into an \( n \)-tuple of the projections of \( x \) along the ordered basis \( b_1, \ldots, b_n \). It is linear since, by \cref{thm:free_module_basis_projections_are_linear}, the projections are linear.

  Now define the inverse function
  \begin{align*}
    &P: F^n \to V \\
    &P(y_1, \ldots, y_n) \coloneqq \sum_{i=1}^n y_i b_i,
  \end{align*}
  which is obviously linear.

  The composition of \( L \) and \( P \) is the identity mapping on \( V \). Indeed, for any \( x \in V \),
  \begin{equation*}
    (P \circ L)(x)
    =
    P(\pi_{b_1}(x), \ldots, \pi_{b_n}(x))
    =
    \sum_{i=1}^n \pi_{b_i}(x) b_i
    =
    x.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:finite_dimensional_operators_are_isomorphic}
  The vector space of all linear maps \( \hom(\R^n, \R^m) \) is isomorphic to the matrix space \( \R^{n \times m} \)
\end{proposition}
