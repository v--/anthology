\subsection{Vector spaces}\label{subsec:vector_spaces}

\begin{definition}\label{def:vector_space}
  A \Def{vector space} \( (V, +, \cdot) \) is a left module\Tinyref{def:left_module} over a field \( F \).

  We call elements of \( F \) \Def{scalars} and elements of \( V \) \Def{vectors}.

  The category of vector spaces over \( F \) is denoted by \( \Cat{Vect}_F \).
\end{definition}

\begin{definition}\label{def:vector_field}
  Let \( V \) be a vector space over \( F \). Functions of the type
  \begin{equation*}
    f: F \to V
  \end{equation*}
  are called \Def{vector fields}. To avoid confusion, \( F \) is sometimes referred to as a \Def{scalar field}. This convention comes from physics and is dominant in areas that are far from algebraic field theory, hence in practice it does not cause a lot of confusion.
\end{definition}

\begin{remark}\label{remark:real_vector_space}
  Outside of algebra, we are usually only interested in vector spaces over the fields \( \R \) or \( \BB{C} \). We call them \Def{real vector spaces} and \Def{complex vector spaces}, respectively.
\end{remark}

\begin{proposition}\label{thm:field_extension_is_vector_space}
  Let \( F \) be a field extension\Tinyref{def:field_extension} of \( G \). Then \( F \) is a vector space over \( G \).
\end{proposition}
\begin{proof}
  Since \( F \) already has the structure of an abelian group, we must only define scalar multiplication
  \begin{align*}
    &\circ: G \times F \to F, \\
    &g \circ f \coloneqq gf,
  \end{align*}
  where the product in the definition is simply multiplication in \( F \). The well-definedness of \( \circ \) follows from the well-definedness of multiplication in \( F \).
\end{proof}

\begin{remark}\label{remark:linear_span_only_for_vector_spaces}
  The definition for linear span\Tinyref{def:linear_span} applies to general commutative modules\Tinyref{def:left_module}. However, since \cref{thm:vector_space_linear_dependence,thm:vector_space_basis} do not apply to general commutative modules, it makes sense to only use linear spans withing the context of vector spaces.
\end{remark}

\begin{definition}\label{def:linear_span}
  For a set \( A \subseteq X \) of vectors, the set of all linear combinations of finite subsets of \( A \) is called its span and is denoted by \( \Span{A} \).
\end{definition}

\begin{proposition}\label{thm:vector_space_linear_dependence}
  The set \( A \subseteq V \) is linearly dependent in the sense of \cref{def:left_module_linear_dependence} if and only if there exists a vector \( x \in V \) such that
  \begin{equation*}
    x \in \Span{A} \setminus \{ x \}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \Implies Let \( A \subseteq M \) and let
  \begin{equation*}
    0_M \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where \( t_1, \ldots, t_n \) have at least one nonzero scalar and where \( x_1, \ldots, x_n \) are nonzero vectors. Without loss of generality, assume that \( t_{n_0} \) is the nonzero scalar. Then
  \begin{align*}
    0_M &= \sum_{k=1}^n t_k x_k, \\
    t_{k_0} x_{k_0} &= -\sum_{k=1}^n t_k x_k, \\
    x_{k_0} &= \sum_{k=1}^n \left(-\frac {t_k} {t_{k_0}} \right) x_k \in \Span{A} \setminus \left\{ x_{k_0} \right\}.
  \end{align*}

  \ImpliedBy Let \( A \subseteq M \) and \( x \in \Span{A} \setminus \{ 0_M, x \} \). By \cref{def:linear_combination}, there exist nonzero vectors \( x_1, \ldots, x_n \in A \) and scalars \( t_1, \ldots, t_n \in R \) such that
  \begin{equation*}
    x \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where at least one of \( t_1, \ldots, t_n \) is nonzero.

  Then \( 0_M \) is a nontrivial linear combination of the nonzero vectors \( x_1, \ldots, x_n, x \):
  \begin{equation*}
    0_M = \sum_{k=1}^n t_k x_k - x.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:vector_space_basis}
  The set \( B \subseteq V \) is a basis in the sense of \cref{def:left_module_basis} if and only if it is linearly independent and
  \begin{equation*}
    V = \Span{B}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \Implies We will prove the contraposition, that is, if \( \Span{B} \neq M \), then \( B \) is not a maximal linearly independent set.

  If \( \Span{B} \subsetneq M \), then there exists a vector \( x \in M \) such that \( x \) is not a linear combination of any subset of \( B \). Thus \( B \cup \{ x \} \) does not have a nontrivial linear combination that equals zero. Hence \( B \cup \{ x \} \) is linearly independent.

  \ImpliedBy Let \( B \subseteq M \) and \( \Span{B} = M \). Assume\LEM that there exists a vector \( x \in M \setminus B \) such that the set \( B \cup \{ x \} \) is linearly independent.

  Then, for any \( b \in B \), the vector \( x + b \) is a linear combination of elements, one of which is independent of \( B \). Thus, by \cref{thm:vector_space_linear_dependence},
  \begin{equation*}
    M = \Span{B} \subsetneq \Span{B \cup \{ x \}} \subseteq M,
  \end{equation*}
  which is a contradiction.
\end{proof}

\begin{theorem}\label{thm:all_vector_spaces_are_free_left_modules}
  All vector spaces have a basis\Tinyref{def:left_module_basis}. Equivalently, all vector spaces are free modules\Tinyref{def:free_left_module}. See \cref{thm:aoc/vector_space_bases}.
\end{theorem}
\begin{proof}
  Let \( V \) be a vector space. Assume that it does not have a basis. Let \( \Cal{B} \) be the family of all linearly independent subsets\Tinyref{def:linear_combination} of \( V \).

  The family \( \Cal{B} \) is obviously nonempty since any singleton\Tinyref{remark:singleton_sets} from \( V \) belongs to \( \Cal{B} \). The union of any chain \( \Cal{B}' \subseteq \Cal{B} \) can then contain only linearly independent elements since otherwise\LEM we would have that some set in \( \Cal{B}' \) is not linearly independent. Thus we can apply Zorn's lemma\Tinyref{thm:aoc/zorn} to obtain a maximal element \( B \).

  Assume\LEM that \( B \) is not a basis, that is,
  \begin{equation*}
    \Span B \subsetneq V.
  \end{equation*}

  Take \( V \in V \setminus \Span B \). Then the set \( B \cup \{ v \} \) is linearly independent, which contradicts the assumption that \( B \) is not a basis. Thus \( B \) is a basis of \( V \) and \( V \) is a free module.
\end{proof}

\begin{definition}\label{def:vector_space_dimension}
  The free module rank\Tinyref{def:free_left_module} of a vector space \( V \) is called the \Def{dimension} \( \dim V \) of \( V \).
\end{definition}

\begin{proposition}\label{thm:linear_maps_form_algebra}
  The set \( \Hom(U, V) \) is a vector space.
\end{proposition}
\begin{proof}
  By \cref{thm:functions_over_ring_form_algebra}, \( \Hom(U, V) \) forms an \( F \)-vector space.
\end{proof}

\begin{remark}\label{remark:functional}
  The term \enquote{functional} does not have a strict meaning. For example, logicians use terms like \enquote{primitive recursive functional} for certain generalized functions. Functions are also ill-defined, see \cref{remark:function_definition}. Outside of logic, however, the term \enquote{functional} usually refers to a function from a vector space \( V \) to its base field \( F \). Examples include linear functionals\Tinyref{def:linear_operator}, like projection maps\Tinyref{def:left_module_basis_projection} and derivatives\Tinyref{def:derivatives}, and nonlinear functionals, like the Minkowski functionals\Tinyref{def:minkowski_functional}.
\end{remark}
