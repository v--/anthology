\subsection{Polynomials}\label{subsec:polynomials}

\begin{remark}\label{remark:polynomial_commutative_ring}
  In this subsection, \( R \) will refer to a nontrivial commutative unital ring\Tinyref{def:semiring/commutative_unital_ring}.
\end{remark}

\begin{definition}\label{def:polynomial}\cite[149]{Knapp2016BAlg}
  A \Def{polynomial} \( p \) over \( R \) is a sequence of members of \( R \) called \Def{coefficients},
  \begin{equation*}
    p \coloneqq ( a_0, a_1, a_2, \ldots ) \subseteq R,
  \end{equation*}
  such that only finitely many coefficients are nonzero.

  \begin{defenum}
    \DItem{def:polynomial/zero_polynomial} An exception to most rules is the \Def{zero polynomial}, all of whose coefficients are zeroes.

    \DItem{def:polynomial/leading_coefficient} The last nonzero coefficient of a nonzero polynomial is called the \Def{leading coefficient} and is denoted by \( \LC(p) \). If \( \LC(p) = 1 \), we call the polynomial \Def{2}.

    \DItem{def:polynomial/degree} The zero-based index of the leading coefficient is called the \Def{degree} of the polynomial as is denoted by \( \deg(p) \). That is, if only \( a_0 \) is nonzero, then \( \deg(p) = a_0 \). The degree of the zero polynomial is left undefined.

    \DItem{def:polynomial/expression} It is conventional to write a polynomial of degree \( k \) as the expression\Tinyref{def:language}
    \begin{equation*}
      p(X) \coloneqq a_k X^k + a_{k-1} X^{k-1} + \ldots + a_2 X^2 + a_1 X + a_0 = \sum_{i=0}^k a_k X^k
    \end{equation*}
    and the zero polynomial as
    \begin{equation*}
      p(X) \coloneqq 0.
    \end{equation*}

    We use capital letters to highlight that this is not a function - see \Tinyref{def:polynomial_function}.

    \DItem{def:polynomial/degree_names} Polynomials of degree \( k \) with special names include
    \begin{itemize}
      \item \Def{constant} for the zero polynomial or if \( k = 0 \).
      \item \Def{linear} if \( k = 1 \).
      \item \Def{quadratic} if \( k = 2 \).
      \item \Def{cubic} if \( k = 3 \).
      \item \Def{quartic} if \( k = 4 \).
      \item \Def{quintic} if \( k = 5 \).
    \end{itemize}
  \end{defenum}
\end{definition}

\begin{definition}\label{def:algebra_of_polynomials}
  Denote by \( R[X] \) the set of polynomials over \( R \). Note that it is bijective with \( c_{00} \) and we can inherit pointwise addition and scalar multiplication from there. That is,

  \begin{defenum}
    \DItem{def:algebra_of_polynomials/addition} We define \Def{polynomial addition} point as
    \begin{align*}
      &+: R[X] \times R[X] \to R[X] \\
      &(a_0, a_1, \ldots) + (b_0, b_1, \ldots) \coloneqq (a_0 + b_0, b_0 + b_1, \ldots)
    \end{align*}

    \DItem{def:algebra_of_polynomials/scalar_multiplication} We define \Def{scalar multiplication} as
    \begin{align*}
      &\cdot: R[X] \times R[X] \to R[X] \\
      &t \cdot (a_0, a_1, \ldots) \coloneqq (t a_0, t b_1, \ldots)
    \end{align*}

    \DItem{def:algebra_of_polynomials/matrix_multiplication} In order to make \( R[X] \) into an algebra\Tinyref{def:algebra_over_ring}, we define \Def{polynomial multiplication} \( \odot: R[X] \times R[X] \to R[X] \) as follows: if \( (a_0, a_1, \ldots) \) and \( (b_0, b_1, \ldots) \) are polynomials, their product is defined to be the polynomial with coefficients
    \begin{equation}
      c_l \coloneqq \sum_{i+j=l} a_i b_j, l = 0, 1, \ldots.
    \end{equation}

    Polynomial multiplication is bilinear, associative and commutative.
  \end{defenum}

  We usually refer to \( R[X] \) as the \Def{polynomial ring}. We will implicitly use the canonical embedding \( \iota: R \to R[X] \), which sends an element \( r \) of \( R \) into the constant polynomial \( p(X) \coloneqq r \).
\end{definition}

\begin{proposition}\label{thm:polynomial_degree_properties}
  The polynomial degree has the following basic properties:
  \begin{thmenum}
    \DItem{thm:polynomial_degree_properties/sum} For nonzero polynomials \( p, q \in R[X] \) with \( p \neq -q \), we have
    \begin{equation*}
      \deg (p + q) \leq \max \{ \deg p, \deg q \}.
    \end{equation*}

    \DItem{thm:polynomial_degree_properties/product} For nonzero polynomials \( p, q \in R[X] \) with \( pq \neq 0 \), we have
    \begin{equation*}
      \deg (pq) \leq \deg p + \deg q,
    \end{equation*}
    with equality holding if \( R \) is an integral domain.

    The requirement that \( pq \neq 0 \) may also be dropped if \( R \) is an integral domain as per \cref{thm:polynomials_over_integral_domain_are_integral_domain}.
  \end{thmenum}
\end{proposition}
\begin{proof}
  Fix nonzero polynomials
  \begin{align*}
    p(X) &\coloneqq \sum_{i=0}^n a_i X^i, \\
    q(X) &\coloneqq \sum_{j=0}^m b_j X^j.
  \end{align*}

  \begin{description}
    \RItem{thm:polynomial_degree_properties/sum} Additionally assume that \( p \neq -q \) since otherwise \( p + q = 0 \) and \( \deg(p + q) \) is undefined. Thus there exists at least one index \( i = 1, 2, \ldots \) so that \( a_i \neq b_i \). Denote by \( k \) the largest such index (only finitely many are nonzero). Then
    \begin{equation*}
      a_i = b_i = 0 \text{ for } i > k.
    \end{equation*}

    Therefore \( \deg(p + q) = k \). Note that \( k \) cannot exceed both \( \deg p \) and \( \deg q \) because it corresponds to a nonzero coefficient. Thus \( k \leq \max\{ \deg p, \deg q \} \).

    \RItem{thm:polynomial_degree_properties/product} By assumption \( pq \neq 0 \) so there exists at least one nonzero coefficient, say \( c_k \). Obviously \( k \leq \deg p + \deg q \) since the highest possible degree of \( pq \) is \( \deg p + \deg q \). Thus
    \begin{equation*}
      k = \deg (pq) \leq \deg p + \deg q.
    \end{equation*}

    If \( R \) is an integral domain, the product of nonzero elements is nonzero. Thus the leading coefficient \( \LC(pq) = \LC(p)\LC(q) \) is nonzero and
    \begin{equation*}
      \deg(pq) = \deg p + \deg q.
    \end{equation*}
  \end{description}
\end{proof}

\begin{proposition}\label{thm:polynomial_ring_universal_property}\cite[150]{Knapp2016BAlg}
  For any nontrivial commutative unital ring \( T \), any unital ring homomorphism \( \varphi: R \to T \) and any \( t \in T \), there exists a unique homomorphism \( \Phi_t: R[X] \to T \) such that \( \iota(1) = t \) and the following diagram commutes:

  \begin{AlignedEquation}\label{thm:polynomial_ring_universal_property/diagram}
    \begin{mplibcode}
      beginfig(1);
        input metapost/graphs;

        v1 := thelabel("$R$", origin);
        v2 := thelabel("$R[X]$", (2, 0) scaled u);
        v3 := thelabel("$T$", (1, -1) scaled u);

        a1 := straight_arc(v1, v2);
        a2 := straight_arc(v1, v3);

        d1 := straight_arc(v2, v3);

        draw_vertices(v);
        draw_arcs(a);
        drawarrow d1 dotted;

        label.top("$\iota$", straight_arc_midpoint of a1);
        label.llft("$\varphi$", straight_arc_midpoint of a2);
        label.lrt("$\Phi_t$", straight_arc_midpoint of d1);
      endfig;
    \end{mplibcode}
  \end{AlignedEquation}

  We will call the map \( \Phi_t \) a \Def{substitution homomorphism}. If \( T = R \), we simply call \( \Phi_r \) it an \Def{evaluation} at \( r \in R \).
\end{proposition}
\begin{proof}
  We will first prove uniqueness. Let \( \Phi_t: R[X] \to T \) and \( \Psi_t: R[X] \to T \) are two such homomorphisms and take their difference \( \Theta_t \coloneqq \Phi_t - \Psi_t \).

  Then for \( r \in R \) we have
  \begin{equation*}
    \Theta_t(\iota(r)) = \Psi_t(\iota(r)) - \Psi_t(\iota(r)) = \varphi(r) - \varphi(r) = 0.
  \end{equation*}

  Now since for any polynomial we have \( p = \iota(1) p \) and since \( \Theta_t \) is a homomorphism of rings, we have
  \begin{equation*}
    \Theta_t(p) = \Theta_t(\iota(1) p) = 0 \Theta_t(p) = 0.
  \end{equation*}

  Thus \( \Theta_t = 0 \) and \( \Phi_t = \Psi_t \).

  Now we will show existence. Take a polynomial
  \begin{equation*}
    p = (a_0, a_1, \ldots, a_n, 0, 0, \ldots).
  \end{equation*}

  Define
  \begin{equation*}
    \Phi_t(p) \coloneqq \sum_{i=1}^n \phi(a_0) t^i.
  \end{equation*}

  By additivity, \( \Phi_t: R[X] \to T \) is obviously a homomorphism and \( \Phi_t((0, 1, 0, \ldots)) = t \). Therefore we have proven existence.
\end{proof}

\begin{proposition}\label{thm:polynomial_ring_units}
  The units of the polynomial ring \( R[X] \) are precisely the units of \( R \).
\end{proposition}
\begin{proof}
  Any unit of \( R \) is obviously a unit of \( R[X] \).

  For the converse, fix a nonzero constant polynomial \( p(X) = r \). In order for it to have an inverse \( q(X) \), we should have
  \begin{equation*}
    1 = p(X) q(X) = r q(X),
  \end{equation*}
  which can only happen if \( q(X) \) is a constant and a multiplicative inverse of \( r \).
\end{proof}

\begin{proposition}\label{thm:polynomials_over_integral_domain_are_integral_domain}
  If \( R \) is an integral domain, the polynomial ring \( R[X] \) is also an integral domain.
\end{proposition}
\begin{proof}
  Polynomial multiplication inherits its commutativity from multiplication in \( R \). It remains only to show that \( R[X] \) has no zero divisors.

  Fix two polynomials \( p, q \in R[X] \). If either of them is zero, their product \( pq \) is zero.

  Assume that both are nonzero polynomials. The leading coefficient of their product is, by definition of multiplication, \( \LC(pq) = \LC(p) \LC(q) \). Since \( R \) has no zero divisors, then \( \LC(pq) \neq 0 \) and thus \( pq \) is a nonzero polynomial.

  Therefore \( R[X] \) is an integral domain.
\end{proof}

\begin{proposition}\label{thm:polynomials_over_integral_domain_are_integral_domain}
  If \( R \) is a unique factorization domain, the polynomial ring \( R[X] \) is also a unique factorization domain.
\end{proposition}

\begin{theorem}[Euclidean division of polynomials]\label{thm:euclidean_division_of_polynomials}\cite[10]{Knapp2016BAlg}
  Let \( a, b \in R[X] \) and \( b \) be monic\Tinyref{def:polynomial/leading_coefficient} (in particular, \( b \neq 0 \)). Then there exist unique polynomials \( q, r \in R[X] \), where \( r \) is either zero or \( \deg r < \deg b \), such that
  \begin{equation*}
    a = bq + r.
  \end{equation*}
\end{theorem}
\begin{proof}
  Let \( a, b \in F[X] \) and \( b \neq 0 \). If \( a = 0 \) or \( \deg a < \deg b \), define
  \begin{align*}
    q &\coloneqq 0, \\
    r &\coloneqq a.
  \end{align*}

  In this case, \( \deg r = \deg a < \deg b \).

  Suppose that \( \deg b \leq \deg a \). We will use proof by induction on \( \deg a \). If \( \deg a = 0 \), obviously \( \deg b = 0 \) (thus \( b = 1 \)) and we define
  \begin{align*}
    q &\coloneqq a, \\
    r &\coloneqq 0.
  \end{align*}

  In this case, \( r \) is the zero polynomial.

  Assume the result holds for \( \deg a < n \) and let \( \deg a = n, \deg b = m \). Then there exists a polynomial \( \hat a(X) \) that is either zero or \( \deg \hat a = n - 1 \) such that
  \begin{equation*}
    a(X) = a_n a^n + \hat a(X).
  \end{equation*}

  Analogously, we find \( \hat b(X) \) that is either zero or \( \deg \hat b = m - 1 \) such that
  \begin{equation*}
    b(X) = X^m + \hat b(X).
  \end{equation*}

  Thus
  \begin{align*}
    \hat r(X)
    &\coloneqq
    a(X) - b(X) a_n X^{n-m}
    = \\ &=
    a_n X^n + \hat a(X) - (b_m X^m + \hat b(X)) a_n X^{n-m}
    = \\ &=
    a_n X^n + \hat a(X) - a_n X^n - \hat b(X) a_n X^{n-m}
    = \\ &=
    \hat a(X) - \hat b(X) a_n X^{n-m}.
  \end{align*}

  Therefore \( \hat r(X) \) is either the zero polynomial (in which case we define \( r(X) \coloneqq \hat r(X) \)) or \( \deg \hat r \leq n - 1 \). In the latter case, we can divide \( \hat r \) by \( b \) to obtain \( \hat q(X) \) and \( r(X) \) such that
  \begin{equation*}
    \hat r(X) \coloneqq b(X) \hat q(X) + r(X),
  \end{equation*}
  where \( r = 0 \) or \( \deg r < \deg b \).

  Substitute into the definition of \( \hat r(X) \):
  \begin{align*}
    \hat r(X) &= a(X) - b(X) a_n X^{n-m} \\
    b(X) \hat q(X) + r(X) &= a(X) - b(X) a_n X^{n-m} \\
    b(X) \left(\hat q(X) - a_n X^{n-m} \right) + r(X) &= a(X).
  \end{align*}

  Define
  \begin{equation*}
    q(X) \coloneqq \hat q(X) - a_n X^{n-m}.
  \end{equation*}

  We have obtained polynomials \( r(X) \) and \( q(X) \) where \( r(X) \) is either zero or \( \deg r < \deg b \).

  It remains only to show uniqueness. Suppose that
  \begin{equation*}
    a = bq + r = bq' + r'.
  \end{equation*}

  If \( r - r' \) is the zero polynomial, so is \( q - q' \) and uniqueness follows.

  If \( r - r' \) is not zero, then neither is \( q - q' \). Then \( b(q - q') = -(r - r') \) and
  \begin{equation*}
    \deg b + \deg(q - q') = \deg[b (q - q')] = \deg(r - r') \leq \max(\deg r, \deg r') < \deg b,
  \end{equation*}
  which is a contradiction.

  This proves uniqueness.
\end{proof}

\begin{corollary}\label{thm:polynomials_over_field_are_euclidean_domain}\cite[10]{Knapp2016BAlg}
  The polynomial ring\Tinyref{def:semiring/integral_domain} \( F[X] \) over a field \( F \) is an Euclidean\Tinyref{def:semiring/euclidean_domain} domain with \( \delta(p) \coloneqq \deg p \). Furthermore, the remainder and quotient are unique.
\end{corollary}
\begin{proof}
  By \cref{thm:polynomials_over_integral_domain_are_integral_domain}, \( F[X] \) is an integral domain.

  To show that it is Euclidean, fix two polynomials \( a, b \in F[X] \) with \( b \neq 0 \). We use \cref{thm:euclidean_division_of_polynomials} to perform Euclidean division of \( a \) by the monic polynomial \( \frac {b} {\LC(b)} \) and obtain polynomials \( q, r \), where either \( r = 0 \) or \( \deg r < \deg b \), such that
  \begin{equation*}
    a = \frac {b} {\LC(b)} q + r.
  \end{equation*}

  Instead of dividing \( b \) by its leading coefficient \( \LC(b) \), we can divide \( q \) and thus obtain the required factorization.
\end{proof}

\begin{definition}\label{def:polynomial_function}
  Let \( \End(R) \) be the endomorphism ring\Tinyref{def:endomorphism_semiring} over \( R \). Define the unital ring homomorphism
  \begin{align*}
    &\Phi: R[X] \to \End(R) \\
    &\Phi((a_0, a_1, \ldots, a_n, 0, 0, \ldots)) \coloneqq \left( x \to \sum_{i=0}^n a_i x^n \right),
  \end{align*}
  which constructs a \Def{polynomial function} from a polynomial. This map is not injective in general and multiple polynomials may be equivalent as functions\Tinyref{def:function}.

  If we want to highlight that we are referring to a polynomial function rather than a polynomial \( p(X) \), we use a lowercase letter for the variable, i.e.
  \begin{equation*}
    p(x) = \sum_{i=0}^n a_i x^i.
  \end{equation*}
\end{definition}
\begin{proof}
  This is indeed a homomorphism. We will only prove that multiplication of polynomials corresponds to multiplication of polynomial functions because the rest is obvious.
  \begin{align*}
    p(X) &\coloneqq \sum_{i=0}^n a_i X^i, \\
    q(X) &\coloneqq \sum_{j=0}^m b_j X^j.
  \end{align*}

  For their product \( s(X) = \sum_{i=0}^{n + m} c_i X^i \) by definition we have
  \begin{equation*}
    c_l = \sum_{i+j=l} a_i b_j, l = 0, 1, \ldots, n + m.
  \end{equation*}

  We need to show that for any \( r \in R \)
  \begin{equation*}
    p(r) q(r) = s(r).
  \end{equation*}

  Using associativity and commutativity of multiplication and distributivity of multiplication over addition, we obtain
  \begin{align*}
    p(r) q(r)
    &=
    \left( \sum_{i=0}^n a_i r^i \right) \left( \sum_{j=0}^m b_j r^j \right)
    = \\ &=
    \sum_{i=0}^n a_i r^i \left( \sum_{j=0}^m b_j r^j \right)
    = \\ &=
    \sum_{i=0}^n a_i r^i \left( \sum_{j=0}^m b_j r^j \right)
    = \\ &=
    \sum_{i=0}^n \sum_{j=0}^m a_i r^i b_j r^j
    = \\ &=
    \sum_{i=0}^n \sum_{j=0}^m a_i b_j r^{i + j}
    = \\ &=
    \sum_{l=0}^{n + m} \sum_{i+j=l} a_i b_j r^l
    = \\ &=
    \sum_{l=0}^{n + m} c_l r^l
    =
    s(r),
  \end{align*}
  where we have used that \( a_i = 0, i > n \) and \( b_j = 0, j > m \).
\end{proof}

\begin{proposition}\label{thm:polynomial_root_iff_divisible}
  The value \( u \in R \) is a root\Tinyref{def:semiring_kernel} of the polynomial function \( p(x) \) if any only if the polynomial \( (X - u) \) divides \( p(X) \).
\end{proposition}
\begin{proof}
  \begin{description}
    \Implies Suppose that \( u \in R \) is a root of \( p(x) \). By \cref{thm:euclidean_division_of_polynomials}, we can divide \( p(X) \) by the monic polynomial \( (X - u) \):
    \begin{equation*}
      p(X) = (X - u) q(X) + r(X).
    \end{equation*}

    Assume\LEM that \( r(X) \) is nonzero. Evaluating \( p(X) \) at \( u \) gives us
    \begin{equation*}
      0 = p(u) = (u - u) q(r) + r(u),
    \end{equation*}
    hence \( u \) is a root of \( r(X) \). But \( \deg r < \deg (X - u) = 1 \), that is, \( r \) is a nonzero constant and cannot have roots. The obtained contradiction proves the statement.

    \ImpliedBy Suppose that \( (X - u) \) divides \( p(X) \) with quotient \( q(X) \). Then
    \begin{equation*}
      p(X) = (X - u) q(X).
    \end{equation*}

    Evaluation at \( u \) gives us
    \begin{equation*}
      p(u) = (u - u) q(u) = 0.
    \end{equation*}

    Therefore \( u \) is a root of \( p(X) \).
  \end{description}
\end{proof}

\begin{definition}\label{def:polynomial_root_multiplicity}
  We say that the polynomial \( p \in R[X] \) has the root \( r \in R \) with multiplicity \( m \in \Z^{>0} \) if there exists a polynomial \( q \in R[X] \) of degree \( \deg q = \deg p - m \) such that
  \begin{equation*}
    p(X) = (X - r)^m q(X).
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:integral_domain_polynomial_root_limit}
  If \( R \) is an integral domain, a nonzero polynomial of degree \( n \) has at most \( n \) (not necessarily distinct\Tinyref{def:polynomial_root_multiplicity}) roots.
\end{proposition}
\begin{proof}
  We will use induction on \( n \).

  In the case \( n = 0 \), we have that \( p \) is a nonzero constant polynomial. Such a polynomial cannot\LEM have roots, hence the statement holds.

  Now assume that the statement holds for polynomials of degrees \( 1, \ldots, n - 1 \). Let \( p \in R[X] \) be a polynomial of degree \( n \) and let \( r \in R \) be a root of \( p \). \Cref{thm:polynomial_root_iff_divisible} implies that there exists a polynomial \( q(X) \) of degree \( n - 1 \) such that
  \begin{equation*}
    p(X) = (X - r) q(X).
  \end{equation*}

  Fix \( t \neq r \) that is a not a root of \( q(X) \). Evaluation at \( t \) gives us
  \begin{equation*}
    p(t) = (t - r) q(t).
  \end{equation*}

  Both \( (t - r) \neq 0 \) and \( q(t) \neq 0 \). Since \( R \) has no zero divisors, the product \( p(t) \) of \( (t - r) \) and \( q(t) \) is also nonzero. Thus the only roots of \( p(X) \) are \( r \) and the roots of \( q(X) \).

  By the induction hypothesis, \( q(X) \) has at most \( n - 1 \) roots (counting multiplicities). Thus \( p(X) \) has at most \( (n - 1) + 1 = n \) roots.
\end{proof}

\begin{proposition}\label{thm:polynomials_have_identical_roots}
  If \( R \) is an integral domain, two polynomials \( p, q \in R[X] \) of the same degree \( n \) are equal if and only if their functions match at \( n + 1 \) points.
\end{proposition}
\begin{proof}
  Define \( r \coloneqq p - q \). This is a polynomial of degree at most \( n \) that has \( n + 1 \) roots. By \cref{thm:integral_domain_polynomial_root_limit}, \( r \) is the zero polynomial. Hence \( p = q \).
\end{proof}

\begin{definition}\label{def:primitive_polynomial}\cite[394]{Knapp2016BAlg}
  A nonzero polynomial is called \Def{primitive} if its coefficients are coprime\Tinyref{def:coprime_ring_ideal}.
\end{definition}

\begin{definition}\label{def:multivariate_polynomial}
  We define the \Def{multivariate polynomial ring} in \( k \) indeterminates as the \enquote{iterated} single-variable polynomial ring
  \begin{equation*}
    R[X_1, X_2, \ldots, X_k] \coloneqq R[X_1][X_2] \cdots [X_k].
  \end{equation*}

  Each polynomial \( p(X_1, \ldots, X_k) \) is an \( k \)-dimensional array\Tinyref{def:array} over \( R \). If there are only two variables, multivariate polynomials are matrices
  \begin{equation*}
    p(X_1, X_2) \coloneqq \begin{pmatrix}
      a_{0,0} & a_{0,1} & \cdots \\
      a_{1,0} & a_{1,1} & \cdots \\
      \vdots  & \vdots  & \ddots \\
    \end{pmatrix}
  \end{equation*}
  with only finitely many nonzero elements.

  A \Def{monomial} is a polynomial with only one nonzero element. The monomial
  \begin{equation*}
    p(X_1, X_2) \coloneqq \begin{pmatrix}
      0       & 0       & 0       & \cdots \\
      0       & 0       & 0       & \cdots \\
      0       & r       & 0       & \cdots \\
      0       & 0       & 0       & \cdots \\
      \vdots  & \vdots  & \vdots  & \ddots \\
    \end{pmatrix}
  \end{equation*}
  can also be written symbolically as \( p(X_1, X_2) = r X_1^2 X_2 \), with the power for each variable corresponding to the zero-based index of the element along the corresponding axis.

  The sum of the indices over all axes is called the \Def{degree} of this monomial. The above monomial has degree \( 3 = 2 + 1 \). We leave the degree for the zero monomial undefined.

  Every polynomial can be regarded as the sum of finitely many monomials by taking each element of the array and putting it in its own monomial array.

  The \Def{degree} \( \deg p \) of a multivariate polynomial \( p \) is defined as the maximal degree among all of its nonzero monomials. If all monomials are zero, the degree is left undefined.

  If all nonzero monomials have the same degree, the polynomial is said to be \Def{homogeneous}.
\end{definition}
